{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec249343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fe233e70-0b8c-4705-abcf-b0ffe9bf74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, params=None):\n",
    "        self.params = params\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Applies the sigmoid function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def negative_log_likelihood(self, X, Y):\n",
    "        \"\"\"Computes the negative log-likelihood.\"\"\"\n",
    "        logits = X @ self.params\n",
    "        return -np.sum(Y * np.log(self.sigmoid(logits)) + (1 - Y) * np.log(1 - self.sigmoid(logits)))\n",
    "\n",
    "    def default_learning_rate(self, X):\n",
    "        \"\"\"Computes the default learning rate using the Hessian approximation.\"\"\"\n",
    "        logits = X @ self.params\n",
    "        sigmoid_values = self.sigmoid(logits)\n",
    "        hessian_approx = np.sum(sigmoid_values * (1 - sigmoid_values))\n",
    "        return 1 / hessian_approx if hessian_approx != 0 else 0.01\n",
    "\n",
    "    def gradient(self, X, Y):\n",
    "        \"\"\"Computes the gradient of the negative log-likelihood.\"\"\"\n",
    "        logits = X @ self.params\n",
    "        errors = self.sigmoid(logits) - Y\n",
    "        return X.T @ errors\n",
    "\n",
    "    def fit(self, X_train, Y_train, learning_rate=None, max_iter=10000, verbose=False):\n",
    "        \"\"\"Fits the model using gradient descent with specified learning rate.\"\"\"\n",
    "        X = X_train.values\n",
    "        Y = Y_train.to_numpy()\n",
    "        n_features = X.shape[1]\n",
    "        self.params = np.zeros(n_features)  # Initialize parameters\n",
    "\n",
    "        if learning_rate is None:\n",
    "            learning_rate = self.default_learning_rate(X)\n",
    "        \n",
    "        for iter in range(max_iter):\n",
    "            grad = self.gradient(X, Y)\n",
    "            self.params -= learning_rate * grad  # Gradient descent update\n",
    "\n",
    "            # Optionally print the loss every 100 iterations\n",
    "            if verbose:\n",
    "                if i % 100 == 0:\n",
    "                    loss = self.negative_log_likelihood(X, Y)\n",
    "                    print(f\"Iteration {i}: Loss = {loss:.4f}\")\n",
    "\n",
    "    def predict_probs(self, X_train):\n",
    "        \"\"\"Predicts probabilities of labels for input data.\"\"\"\n",
    "        return self.sigmoid(X_train @ self.params)\n",
    "\n",
    "    def predict(self, X_train, threshold=0.5):\n",
    "        \"\"\"Predicts class labels for input data with a given threshhold.\"\"\"\n",
    "        return (self.predict_probs(X_train) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d65c4845-0f73-40c1-8816-5a72e00f3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(logistic_regression_model, n_samples=1000, N_columns=2, true_params=None, \n",
    "         learning_rate=None, max_iter=1000, random_seed=None):\n",
    "    \"\"\"Function testing the accuracy of the logistic regression class\"\"\"\n",
    "    random_state = np.random.default_rng(random_seed)\n",
    "    X_train = pd.DataFrame(random_state.random((n_samples, N_columns)))\n",
    "    X_train[\"constant\"] = 1\n",
    "    if not true_params:\n",
    "        true_params = random_state.random(N_columns+1)\n",
    "    Y_train = pd.Series((random_state.random(n_samples) < 1 / (1 + np.exp(-(X_train @ true_params)))).astype(int))\n",
    "\n",
    "    # Fit the logistic regression model\n",
    "    model = logistic_regression_model()\n",
    "    model.fit(X_train, Y_train, learning_rate=learning_rate, max_iter=max_iter)\n",
    "\n",
    "    print(f\"Correct parameters: {true_params}, Fitted parameters: {model.params}\")\n",
    "\n",
    "    predictions = model.predict(X_train)\n",
    "    print(f\"Model's accuracy:{np.mean(predictions == Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a62f5b19-b5e0-4f06-ac44-045dfb4096b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct parameters: [0.28417258 0.00980147 0.11038067], Fitted parameters: [0.54393037 0.20567226 0.02519313]\n",
      "Model's accuracy:0.598\n"
     ]
    }
   ],
   "source": [
    "test(LogisticRegression, n_samples=1000, N_columns=2, max_iter=10000, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384be91b-402a-45fe-b088-f3135a799368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253cbbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
